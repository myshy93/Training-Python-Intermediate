{
 "cells": [
  {
   "source": [
    "# Lecture 5 - Unit testing\n",
    "\n",
    "## 📕 Today's Agenda\n",
    "---\n",
    " * [Unittest module](#Unittest-module)\n",
    " * [Simple unit test](#Simple-unit-test)\n",
    " * [Executing tests](#Executing-tests)\n",
    " * [Test methods](#Test-methods)\n",
    " * [Setup and teardown](#Setup-and-teardown)\n",
    " * [Mocking](#Mocking)\n",
    " * [Patching](#Patching)\n",
    " * [Skipping tests and errors](#Skipping-tests-and-errors)\n",
    "\n",
    "## 🧪 Theory\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Unittest module\n",
    "\n",
    "There are many modules for testing Python code, one on we will focus on is `unittest`. Other well known module for testing is `pytest`.\n",
    "Unittest is a entire framework for testing your Py code, you can make tests, test suites and run tests with built-in runner.\n",
    "\n",
    "\n",
    "### Simple unit test\n",
    "\n",
    "Let's build a simple test for a simple function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from math import pi\n",
    "import unittest\n",
    "\n",
    "def circle_area(radius):\n",
    "    if not isinstance(radius, float) or not isinstance(radius, int):\n",
    "        raise TypeError('Radius must be integer of float.')\n",
    "    if radius <= 0:\n",
    "        raise ValueError('Radius must be greater then zero.')\n",
    "    return pi * radius ** 2\n",
    "\n",
    "class TestMyFunctions(unittest.TestCase):\n",
    "    def test_circle_area(self):\n",
    "        with self.assertRaises(TypeError):\n",
    "            circle_area(None)\n",
    "        with self.assertRaises(ValueError):\n",
    "            circle_area(-1)\n",
    "        self.assertEqual(circle_area(1), 0.7853981634)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First steps to consider when creating tests:\n",
    "1. Create a class that extends `unittest.TestCase'. Name this class prefixed with 'Test'.\n",
    "2. Create functions in this class for testing functions. Each test function name must be prefixed with 'test_'.\n",
    "3. Write test code in test functions.\n",
    "\n",
    "NOTE: Prefixing classes and functions names will help test framework to auto-discover test.\n",
    "\n",
    "### Executing tests\n",
    "\n",
    "To run a python file with tests, place this snippet at the bottom.\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "```\n",
    "\n",
    "### Test methods\n",
    "The `TestCase` class provides several assert methods to check for and report failures. The following table lists the most\n",
    "commonly used methods (see the tables below for more assert methods):\n",
    "\n",
    "| Method | Checks that |\n",
    "| :-: | :-: |\n",
    "| assertEqual(a, b) | a == b |\n",
    "| assertNotEqual(a, b) | a != b |\n",
    "| assertTrue(x) | x is True |\n",
    "| assertFalse(x) | x is False |\n",
    "| assertIs(a, b) | a is b |\n",
    "| assertIsNot(a, b) | a is not b |\n",
    "| assertIsNone(x) | x is None |\n",
    "| assertIsNotNone(x) | x is not None |\n",
    "| assertIn(a, b) | a in b |\n",
    "| assertNotIn(a, b) | a not in b |\n",
    "| assertIsInstance(a, b) | a is instance of b |\n",
    "| assertNotIsInstance(a, b) | a is not instance of b |\n",
    "| assertRaises(x)* | check if x is raised |\n",
    "\n",
    "\\* Used only with context manager\n",
    "\n",
    "### Setup and teardown\n",
    "\n",
    "The `TestCase` class exposes `setUp` and `tearDown` methods for housekeeping purposes. They are not implemented so you\n",
    "have to do it in your test case.\n",
    "\n",
    "In the *setUp* method you can place code to be run before each test executes and in *tearDown* method you can place code to be\n",
    "that will be executed after each test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class TestMx(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        print('Executing new test...')\n",
    "\n",
    "    def test_one(self):\n",
    "        self.assertEquals(1, 1)\n",
    "\n",
    "    def test_two(self):\n",
    "        self.assertEqual(1, 1)\n",
    "\n",
    "    def tearDown(self):\n",
    "        print('DONE')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mocking\n",
    "\n",
    "From testing perspective mocking is the act of crating dummy objects who emulates an object needed by a function.\n",
    "If you have a functions which extracts information from a HTTP response object, and you want to test that function, you will need\n",
    "a HTTP response object in different states. Because you want to test your function for all possible responses.\n",
    "So you will need a good HTTP response object and may bad responses. You don't want to stress the production server with your\n",
    "requests or to make tricks so you will obtain the needed response type."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class HttpError(Exception):\n",
    "    pass\n",
    "\n",
    "class HttpResponse:\n",
    "    def __init__(self):\n",
    "        self.status = None\n",
    "\n",
    "    def json(self):\n",
    "        return {}\n",
    "\n",
    "def store_car_info(http_response):\n",
    "    \"\"\"This function stores car info data received from http response to disk.\"\"\"\n",
    "    if not isinstance(http_response, HttpResponse):\n",
    "        raise TypeError('HttpResponse expected, got other type.')\n",
    "    if http_response.status == 200:\n",
    "        content = http_response.json()\n",
    "        car_id = content.get('car_id')\n",
    "        if car_id:\n",
    "            print(f'Storing data for {car_id} ...')\n",
    "            # suppose you store data\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ValueError('Invalid http content.')\n",
    "    else:\n",
    "        raise HttpError(f'Http error, status: {http_response.status}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, we got `store_car_function` and has to be tested. **Let's define tests to perform!**\n",
    "\n",
    "Import `Mock` class and make a mock object that emulates a HttpResponse object."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import Mock\n",
    "\n",
    "mock_response = Mock(spec_set=HttpResponse)\n",
    "print(isinstance(mock_response, HttpResponse))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The *Mock* class allow us to define any attribute or method. You can specify values for attributes,\n",
    "return values for methods or errors to raise.\n",
    "\n",
    "**Set value for attributes**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "mock_response2 = Mock()\n",
    "mock_response2.status = 200\n",
    "print(mock_response2.status)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Set return value for methods**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'car_id': 2}\n"
     ]
    }
   ],
   "source": [
    "mock_response2.json.return_value = {'car_id': 2}\n",
    "print(mock_response2.json())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Raise errors**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error raised by mock\n"
     ]
    }
   ],
   "source": [
    "mock_response2.json.side_effect = TypeError('Error raised by mock')\n",
    "try:\n",
    "    mock_response2.json()\n",
    "except TypeError as err:\n",
    "    print(err)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Patching\n",
    "\n",
    "Well, to test a function that has external dependencies is quite easy, just use Mocking, but how you test a function that has\n",
    "has dependencies on external libraries, like datetime for example. How you test a function that internally grabs the datetime\n",
    "and makes some decisions.\n",
    "\n",
    "Like a function for greeting:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def greet():\n",
    "    now = datetime.now()\n",
    "    if 5 < now.hour <= 10:\n",
    "        return 'Good morning!'\n",
    "    elif 10 < now.hour <= 19:\n",
    "        return 'Good afternoon!'\n",
    "    else:\n",
    "        return 'Good evening!'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To test the `greet` function, we have to change the return value for `datetime.now()` temporally. We can do this by using `patch` function.\n",
    "The best way of using it is in a context manager."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "class TestMyFunctions(unittest.TestCase):\n",
    "    def test_greet(self):\n",
    "        # test for evening\n",
    "        with patch('.datetime') as mock:\n",
    "            mock.now.return_value = datetime(2020, 1, 1, 20, 20)\n",
    "            self.assertEqual('Good evening!', greet())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's break it down, so with `patch('.datetime') as mock` we create a mock for built-in module datetime that was imported in our script\n",
    "and set a return value for `now()` method of datetime module. The new value is used just in the actual context.\n",
    "\n",
    "### Skipping tests and errors\n",
    "Unittest supports skipping individual test methods and even whole classes of tests. In addition, it supports marking a\n",
    "test as an “expected failure,” a test that is broken and will fail, but shouldn’t be counted as a failure on a TestResult.\n",
    "\n",
    "Skipping a test is simply a matter of using the `skip()` **decorator** or one of its conditional variants.\n",
    "Unittest supports skipping individual test methods and even whole classes of tests. In addition, it supports marking a\n",
    "test as an “expected failure,” a test that is broken and will fail, but shouldn’t be counted as a failure on a TestResult."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "class MyTestCase(unittest.TestCase):\n",
    "\n",
    "    @unittest.skip(\"demonstrating skipping\")\n",
    "    def test_nothing(self):\n",
    "        self.fail(\"shouldn't happen\")\n",
    "\n",
    "    @unittest.skipIf(sys.version < '3',\n",
    "                     \"not supported in this Python version\")\n",
    "    def test_format(self):\n",
    "        # Tests that work for only a certain version of the library.\n",
    "        pass\n",
    "\n",
    "    @unittest.skipUnless(sys.platform.startswith(\"win\"), \"requires Windows\")\n",
    "    def test_windows_support(self):\n",
    "        # windows specific testing code\n",
    "        pass\n",
    "\n",
    "    def test_maybe_skipped(self):\n",
    "        # test code that depends on the external resource\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To skip a class of tests just decorate the class with `@unittest.skip`, and provide message as argument.                                                                                                                                                                                 Skipping a test is simply a matter of using the skip() decorator or one of its conditional variants, calling TestCase.skipTest() within a setUp() or test method, or raising SkipTest directly.\n",
    "\n",
    "When you expect a test to fail, just decorate test function with `@unittest.expectFailure` and that test will be marked as\n",
    "*expected failure* in test report. If a test that is expected to fail, succeeds it will be marked as *unexpected success*.\n",
    "\n",
    "## 👩‍💻 Practice\n",
    "---\n",
    "1. Define test that have to be run for `store_car_info`.\n",
    "2. Write unit tests for `store_car_info` function. Use Mock.\n",
    "\n",
    "## 🏠 Homework\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}